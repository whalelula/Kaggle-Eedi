{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1\\Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T03:36:23.966045Z",
     "iopub.status.busy": "2024-12-29T03:36:23.965629Z",
     "iopub.status.idle": "2024-12-29T03:36:40.469428Z",
     "shell.execute_reply": "2024-12-29T03:36:40.468157Z",
     "shell.execute_reply.started": "2024-12-29T03:36:23.966015Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# %pip install openai\n",
    "# %pip install pandas\n",
    "# %pip install scikit-learn\n",
    "# %pip install matplotlib\n",
    "# %pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-29T03:36:40.471934Z",
     "iopub.status.busy": "2024-12-29T03:36:40.471472Z",
     "iopub.status.idle": "2024-12-29T03:36:40.478123Z",
     "shell.execute_reply": "2024-12-29T03:36:40.476904Z",
     "shell.execute_reply.started": "2024-12-29T03:36:40.471902Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import kagglehub\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T03:36:40.481143Z",
     "iopub.status.busy": "2024-12-29T03:36:40.480696Z",
     "iopub.status.idle": "2024-12-29T03:36:40.566327Z",
     "shell.execute_reply": "2024-12-29T03:36:40.565075Z",
     "shell.execute_reply.started": "2024-12-29T03:36:40.481113Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "datagen_model = \"gpt-4o\"\n",
    "api_key = 'xxx' \n",
    "\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", api_key))\n",
    "\n",
    "ClusterSize = 5 # For each gpt request, access BatchSize misconceptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2\\MisconceptionName Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T03:36:40.568313Z",
     "iopub.status.busy": "2024-12-29T03:36:40.568015Z",
     "iopub.status.idle": "2024-12-29T03:36:40.612218Z",
     "shell.execute_reply": "2024-12-29T03:36:40.611234Z",
     "shell.execute_reply.started": "2024-12-29T03:36:40.568273Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# data_dir = \"/kaggle/input/eedi-mining-misconceptions-in-mathematics\"\n",
    "data_dir = \"D:\\VSCode\\Kaggle_eedi_sorted\\data\"\n",
    "df = pd.read_csv(os.path.join(data_dir, \"train.csv\"))\n",
    "content_df = pd.read_csv(os.path.join(data_dir, \"misconception_mapping.csv\"))\n",
    "\n",
    "content_df[\"MisconceptionId\"] = content_df[\"MisconceptionId\"].astype(str)\n",
    "id2name = dict(zip(content_df[\"MisconceptionId\"], content_df[\"MisconceptionName\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i= 0\n",
    "# print('ConstructName:',df.loc[i,'ConstructName'])\n",
    "# print('SubjectName:',df.loc[i,'SubjectName'])\n",
    "# print('QuestionText:',df.loc[i,'QuestionText'])\n",
    "# print('AnswerAText:',df.loc[i,'AnswerAText'])\n",
    "# print('AnswerBText:',df.loc[i,'AnswerBText'])\n",
    "# print('AnswerCText:',df.loc[i,'AnswerCText'])\n",
    "# print('AnswerDText:',df.loc[i,'AnswerDText'])\n",
    "# print('CorrectAnswer:',df.loc[i,'CorrectAnswer'])\n",
    "# print('MisconceptionAName:',df.loc[i,'MisconceptionAName'])\n",
    "# print('MisconceptionBName:',df.loc[i,'MisconceptionBName'])\n",
    "# print('MisconceptionCName:',df.loc[i,'MisconceptionCName'])\n",
    "# print('MisconceptionDName:',df.loc[i,'MisconceptionDName'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T03:36:40.613544Z",
     "iopub.status.busy": "2024-12-29T03:36:40.613268Z",
     "iopub.status.idle": "2024-12-29T03:36:40.624051Z",
     "shell.execute_reply": "2024-12-29T03:36:40.622894Z",
     "shell.execute_reply.started": "2024-12-29T03:36:40.613521Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "misconception_ids = set()\n",
    "for col in [\"MisconceptionAId\", \"MisconceptionBId\", \"MisconceptionCId\", \"MisconceptionDId\"]:\n",
    "    misconception_ids.update(df[col].dropna().astype(int).unique())\n",
    "misconception_ids = list(map(str, misconception_ids))\n",
    "print(f\"Number of unique MisconceptionIds: {len(misconception_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T03:36:40.625318Z",
     "iopub.status.busy": "2024-12-29T03:36:40.625027Z",
     "iopub.status.idle": "2024-12-29T03:36:40.646929Z",
     "shell.execute_reply": "2024-12-29T03:36:40.645766Z",
     "shell.execute_reply.started": "2024-12-29T03:36:40.625295Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "missing_ids = content_df[~content_df[\"MisconceptionId\"].isin(misconception_ids)][\"MisconceptionId\"].unique().tolist()\n",
    "len(missing_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T03:36:40.648580Z",
     "iopub.status.busy": "2024-12-29T03:36:40.648206Z",
     "iopub.status.idle": "2024-12-29T03:36:40.666994Z",
     "shell.execute_reply": "2024-12-29T03:36:40.665835Z",
     "shell.execute_reply.started": "2024-12-29T03:36:40.648539Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "all_ids = missing_ids + misconception_ids\n",
    "len(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T03:36:40.669727Z",
     "iopub.status.busy": "2024-12-29T03:36:40.669447Z",
     "iopub.status.idle": "2024-12-29T03:36:40.690738Z",
     "shell.execute_reply": "2024-12-29T03:36:40.689300Z",
     "shell.execute_reply.started": "2024-12-29T03:36:40.669704Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "misconception_ids = set()\n",
    "for col in [\"MisconceptionAId\", \"MisconceptionBId\", \"MisconceptionCId\", \"MisconceptionDId\"]:\n",
    "    misconception_ids.update(df[col].dropna().astype(int).unique())\n",
    "misconception_ids = list(map(str, misconception_ids))\n",
    "print(f\"Number of unique MisconceptionIds: {len(misconception_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T03:36:40.692506Z",
     "iopub.status.busy": "2024-12-29T03:36:40.692212Z",
     "iopub.status.idle": "2024-12-29T03:36:40.727697Z",
     "shell.execute_reply": "2024-12-29T03:36:40.726655Z",
     "shell.execute_reply.started": "2024-12-29T03:36:40.692477Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = df.join(content_df,on='MisconceptionAId')\n",
    "df = df.drop('MisconceptionId',axis=1)\n",
    "df.columns.values[-1] = 'MisconceptionAName'\n",
    "df = df.join(content_df,on='MisconceptionBId')\n",
    "df = df.drop('MisconceptionId',axis=1)\n",
    "df.columns.values[-1] = 'MisconceptionBName'\n",
    "df = df.join(content_df,on='MisconceptionCId')\n",
    "df = df.drop('MisconceptionId',axis=1)\n",
    "df.columns.values[-1] = 'MisconceptionCName'\n",
    "df = df.join(content_df,on='MisconceptionDId')\n",
    "df = df.drop('MisconceptionId',axis=1)\n",
    "df.columns.values[-1] = 'MisconceptionDName'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_df[~content_df[\"MisconceptionId\"].isin(misconception_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3\\ Get similar present misconceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- find most similar misconceptions for missing misconceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 加载 BGE 模型\n",
    "model = SentenceTransformer(\"BAAI/bge-large-en-v1.5\")\n",
    "\n",
    "# 假设有两个 Misconception 的文本描述\n",
    "misconception_text_1 = \"Assumes a sequence is linear\"\n",
    "misconception_text_2 = \"Thinks terms in linear sequence are in direct proportion\"\n",
    "\n",
    "# 生成 embedding\n",
    "emb1 = model.encode(misconception_text_1, normalize_embeddings=True)\n",
    "emb2 = model.encode(misconception_text_2, normalize_embeddings=True)\n",
    "\n",
    "# 计算余弦相似度\n",
    "similarity = cosine_similarity([emb1], [emb2])[0][0]\n",
    "print(f\"Similarity: {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 获取所有文本\n",
    "missing_texts = content_df[content_df['MisconceptionId'].isin(missing_ids)]['MisconceptionName'].tolist()\n",
    "present_texts = content_df[content_df['MisconceptionId'].isin(misconception_ids)]['MisconceptionName'].tolist()\n",
    "\n",
    "# 2. 批量生成 embedding\n",
    "missing_embeddings = model.encode(missing_texts, normalize_embeddings=True, batch_size=32)\n",
    "present_embeddings = model.encode(present_texts, normalize_embeddings=True, batch_size=32)\n",
    "\n",
    "# 3. 计算相似度矩阵\n",
    "similarity_matrix = cosine_similarity(missing_embeddings, present_embeddings)\n",
    "\n",
    "# 4. 获取TOP K相似的misconceptions\n",
    "top_indices_matrix = np.argsort(-similarity_matrix, axis=1)[:, :ClusterSize-1]  # 从大到小\n",
    "\n",
    "missing_similar_misconceptions = {}\n",
    "for i, missing_id in enumerate(missing_ids):\n",
    "    top_misconception_ids = [misconception_ids[j] for j in top_indices_matrix[i]]\n",
    "    missing_similar_misconceptions[missing_id] = top_misconception_ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 900\n",
    "print(\"Missing Misconception:\",content_df.loc[content_df['MisconceptionId']==missing_ids[i],'MisconceptionName'].item())\n",
    "for j in range(ClusterSize-1):\n",
    "    print(\"Similar Misconceptions:\",content_df.loc[content_df['MisconceptionId']==missing_similar_misconceptions[missing_ids[i]][j],'MisconceptionName'].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4\\prompt and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mcq_json(mid,df):\n",
    "    # 找到包含某misconception的MCQ\n",
    "    for i in range(len(df)):\n",
    "        filtered_df = df.loc[(\n",
    "    (df['MisconceptionAId'] == float(mid)) |(df['MisconceptionBId'] == float(mid)) |(df['MisconceptionCId'] == float(mid)) |(df['MisconceptionDId'] == float(mid))\n",
    "    ),['SubjectName','ConstructName','QuestionText','AnswerAText','MisconceptionAName','AnswerBText','MisconceptionBName',\\\n",
    "        'AnswerCText','MisconceptionCName','AnswerDText','MisconceptionDName']]\n",
    "    # 从 filtered_df 中随机抽取一行\n",
    "    mcq_json = filtered_df.sample(n=1, random_state=42).iloc[0].to_json(orient='index')  # random_state 可保证可复现\n",
    "    return mcq_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T03:36:40.741241Z",
     "iopub.status.busy": "2024-12-29T03:36:40.740893Z",
     "iopub.status.idle": "2024-12-29T03:36:40.766279Z",
     "shell.execute_reply": "2024-12-29T03:36:40.765082Z",
     "shell.execute_reply.started": "2024-12-29T03:36:40.741214Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_res(mids,df):\n",
    "  # mids，第一条是missing_misconception_id，其他是present最相关的misconception_id\n",
    "    misconception_rows = []\n",
    "    mcq_rows = []\n",
    "    missing_misconception = content_df.loc[content_df['MisconceptionId']==mids[0],'MisconceptionName'].item()\n",
    "    for m in mids:\n",
    "        misconception_rows.append(str(mids.index(m)+1) + '. ' +content_df.loc[content_df['MisconceptionId']==m,'MisconceptionName'].item())\n",
    "        if mids.index(m)>0:\n",
    "          mcq_rows.append(get_mcq_json(m,df)) #以字符串形式输出json\n",
    "    cluster_misconceptions = '\\n'.join(misconception_rows)\n",
    "    print(cluster_misconceptions)\n",
    "    reference_mcqs = '\\n'.join(mcq_rows) #input\n",
    "    print(reference_mcqs)\n",
    "    num_mcqs = 2 # 每个missing_misconception 生成num_mcqs个MCQs\n",
    "    \n",
    "    prompt = f\"\"\"Your task is to generate Multiple Choice Questions (MCQs) that diagnose the following misconceptions:\n",
    "<misconceptions>\n",
    "{cluster_misconceptions}\n",
    "</misconceptions>\n",
    "\n",
    "Here are reference MCQs that demonstrate the questions and misconceptions:\n",
    "<reference_mcqs>\n",
    "{reference_mcqs}\n",
    "</reference_mcqs>\n",
    "\n",
    "First, in your first key-value pair (key is \"reference_analysis\"), analyze the reference MCQs carefully:\n",
    "1. Understand how to derive the correct answer and how the wrong answers map to respective misconceptions\n",
    "2. Note the style, difficulty level, and precision of language used\n",
    "\n",
    "Then, in your first key-value pair (key is \"MCQ_generation\"), generate {num_mcqs} new MCQs that diagnose this conception \"{missing_misconception}\" , following these guidelines:\n",
    "1. brainstorm mathematical contexts where this misconception commonly appears\n",
    "2. generate the problem, solve it yourself, design 1 right answer, and 3 wrong answers leading to specific misconceptions\n",
    "3. Make questions challenging enough that students must demonstrate real understanding\n",
    "4. ensure the answers are plausible and stem from genuine misconceptions, not careless errors\n",
    "5. Use precise mathematical language matching the style of reference MCQs\n",
    "6. Keep the construct name and subject name as short as possible\n",
    "7. Follow the json format of reference MCQs.\n",
    "\n",
    "\n",
    "Additionaly, for each answer of a MCQ, add a key-value pair and write down your reasoning process.\n",
    "For example: {{'SubjectName':'write subject of this question','ConstructName':'write construct of this question','QuestionText':'write question text',\\\n",
    "'MisconceptionAName':'write down a misconception related to this question as the misconceptionA ','ReasoningA':'write down how misconceptionA leads to answerA','AnswerAText':'write down the answer misonceptionA leads to',\\\n",
    "'MisconceptionBName':'write down a misconception related to this question as the misconceptionB ','ReasoningB':'write down how misconceptionB leads to answerB','AnswerBText':'write down the answer misonceptionB leads to',\\\n",
    "'MisconceptionCName':'write down a misconception related to this question as the misconceptionC ','ReasoningC':'write down how misconceptionC leads to answerC','AnswerCText':'write down the answer misonceptionC leads to',\\\n",
    "'MisconceptionDName':'write down a misconception related to this question as the misconceptionD ','ReasoningD':'write down how misconceptionD leads to answerD','AnswerDText':'write down the answer misonceptionD leads to',\\\n",
    "'CorrectAnswer':'write the correct answer(A/B/C/D)','CorrectAnswerReasoning':'write down why the correct answer is correct'}}\n",
    "    \"\"\"\n",
    "    print(prompt)\n",
    "    response = client.chat.completions.create(\n",
    "    model=datagen_model,\n",
    "    temperature=0.3,\n",
    "    response_format={\"type\": \"json_object\"},\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to generate synthetic data.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "      ]\n",
    "    )\n",
    "    res = response.choices[0].message.content\n",
    "    return res\n",
    "\n",
    "# V2：\n",
    "# V3：去掉3. Include other misconceptions mentioned in the <misconceptions> tag in wrong choices if related这个要求\n",
    "# V4：8. If possible, the news generated {num_mcqs} MCQs should be different from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_res_o1(mids,df):\n",
    "#   # mids，第一条是missing_misconception_id，其他是present最相关的misconception_id\n",
    "#     misconception_rows = []\n",
    "#     mcq_rows = []\n",
    "#     missing_misconception = content_df.loc[content_df['MisconceptionId']==mids[0],'MisconceptionName'].item()\n",
    "#     for m in mids:\n",
    "#         misconception_rows.append(str(mids.index(m)+1) + '. ' +content_df.loc[content_df['MisconceptionId']==m,'MisconceptionName'].item())\n",
    "#         if mids.index(m)>0:\n",
    "#           mcq_rows.append(get_mcq_json(m,df)) #以字符串形式输出json\n",
    "#     cluster_misconceptions = '\\n'.join(misconception_rows)\n",
    "#     print(cluster_misconceptions)\n",
    "#     reference_mcqs = '\\n'.join(mcq_rows) #input\n",
    "#     print(reference_mcqs)\n",
    "#     num_mcqs = 2 # 每个missing_misconception 生成num_mcqs个MCQs\n",
    "    \n",
    "#     prompt = f\"\"\"Your task is to generate Multiple Choice Questions (MCQs) that diagnose the following misconceptions:\n",
    "# <misconceptions>\n",
    "# {cluster_misconceptions}\n",
    "# </misconceptions>\n",
    "\n",
    "# Here are reference MCQs that demonstrate the questions and misconceptions:\n",
    "# <reference_mcqs>\n",
    "# {reference_mcqs}\n",
    "# </reference_mcqs>\n",
    "\n",
    "# First, in your first key-value pair (key is \"reference_analysis\"), analyze the reference MCQs carefully:\n",
    "# 1. Understand how to derive the correct answer and how the wrong answers map to respective misconceptions\n",
    "# 2. Note the style, difficulty level, and precision of language used\n",
    "\n",
    "# Then, in your first key-value pair (key is \"MCQ_generation\"), generate {num_mcqs} new MCQs that diagnose this conception \"{missing_misconception}\" , following these guidelines:\n",
    "# 1. brainstorm mathematical contexts where this misconception commonly appears\n",
    "# 2. generate the problem, solve it yourself, design 1 right answer, and 3 wrong answers leading to specific misconceptions\n",
    "# 3. Make questions challenging enough that students must demonstrate real understanding\n",
    "# 4. ensure the answers are plausible and stem from genuine misconceptions, not careless errors\n",
    "# 5. Use precise mathematical language matching the style of reference MCQs\n",
    "# 6. Keep the construct name and subject name as short as possible\n",
    "# 7. Follow the json format of reference MCQs.\n",
    "\n",
    "\n",
    "# Additionaly, for each answer of a MCQ, add a key-value pair and write down your reasoning process.\n",
    "# For example: {{'SubjectName':'write subject of this question','ConstructName':'write construct of this question','QuestionText':'write question text',\\\n",
    "# 'MisconceptionAName':'write down a misconception related to this question as the misconceptionA ','ReasoningA':'write down how misconceptionA leads to answerA','AnswerAText':'write down the answer misonceptionA leads to',\\\n",
    "# 'MisconceptionBName':'write down a misconception related to this question as the misconceptionB ','ReasoningB':'write down how misconceptionB leads to answerB','AnswerBText':'write down the answer misonceptionB leads to',\\\n",
    "# 'MisconceptionCName':'write down a misconception related to this question as the misconceptionC ','ReasoningC':'write down how misconceptionC leads to answerC','AnswerCText':'write down the answer misonceptionC leads to',\\\n",
    "# 'MisconceptionDName':'write down a misconception related to this question as the misconceptionD ','ReasoningD':'write down how misconceptionD leads to answerD','AnswerDText':'write down the answer misonceptionD leads to',\\\n",
    "# 'CorrectAnswer':'write the correct answer(A/B/C/D)','CorrectAnswerReasoning':'write down why the correct answer is correct'}}\n",
    "#     \"\"\"\n",
    "#     response = client.responses.create(\n",
    "#     model=\"o1\",\n",
    "#     reasoning={\"effort\": \"medium\"},\n",
    "#     input=[\n",
    "#         {\"role\": \"system\", \"content\": \"You are a precise data generation assistant for diagnostic MCQs.\"},\n",
    "#         {\"role\": \"user\", \"content\": prompt}\n",
    "#     ]\n",
    "#     )\n",
    "\n",
    "#     res = response.choices[0].message.content\n",
    "#     return res\n",
    "\n",
    "# # V2：\n",
    "# # V3：去掉3. Include other misconceptions mentioned in the <misconceptions> tag in wrong choices if related这个要求\n",
    "# # V4：8. If possible, the news generated {num_mcqs} MCQs should be different from each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5\\ Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with open('misconceptions_clustered.csv', 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    missing_misconception_clustered = pd.read_csv(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_misconception_clustered.sort_values(by='ClusterId',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_misconceptions_df = (\n",
    "    missing_misconception_clustered\n",
    "    .groupby('ClusterId', group_keys=False)\n",
    "    .apply(lambda x: x.sample(n=2, random_state=42))  # random_state 保证可复现\n",
    "    .reset_index(drop=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_misconceptions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_list = []\n",
    "errer_res_list = []\n",
    "syn_df_list = []\n",
    "misconception_ids = [int(x) for x in misconception_ids]\n",
    "missing_ids = [int(x) for x in missing_ids]\n",
    "syn_df = pd.DataFrame({'SubjectName':[],'ConstructName':[],'QuestionText':[],\\\n",
    "    'AnswerAText':[],'MisconceptionAName':[],'ReasoningA':[],\\\n",
    "    'AnswerBText':[],'MisconceptionBName':[],'ReasoningB':[],\\\n",
    "    'AnswerCText':[],'MisconceptionCName':[],'ReasoningC':[],\\\n",
    "    'AnswerDText':[],'MisconceptionDName':[],'ReasoningD':[],\\\n",
    "    'CorrectAnswer':[],'CorrectAnswerReasoning':[]})\n",
    "\n",
    "for i in range(len(evaluation_misconceptions_df)):\n",
    "    mids = str(int(evaluation_misconceptions_df.loc[i,'MisconceptionId'].item()))\n",
    "    rids = missing_similar_misconceptions[mids]\n",
    "    res = json.loads(get_res([mids]+rids,df))\n",
    "    res_list.append(res) # 保存原始的输出结果\n",
    "    reference_analysis, mcq_data = res['reference_analysis'], res['MCQ_generation']\n",
    "    try:\n",
    "        for mcq in mcq_data:\n",
    "            row = {\n",
    "                'SubjectName': mcq['SubjectName'],\n",
    "                'ConstructName': mcq['ConstructName'],\n",
    "                'QuestionText': mcq['QuestionText'],\n",
    "                'AnswerAText': mcq['AnswerAText'],\n",
    "                'MisconceptionAName': mcq['MisconceptionAName'],\n",
    "                'ReasoningA': mcq['ReasoningA'],\n",
    "                'AnswerBText': mcq['AnswerBText'],\n",
    "                'MisconceptionBName': mcq['MisconceptionBName'],\n",
    "                'ReasoningB': mcq['ReasoningB'],\n",
    "                'AnswerCText': mcq['AnswerCText'],\n",
    "                'MisconceptionCName': mcq['MisconceptionCName'],\n",
    "                'ReasoningC': mcq['ReasoningC'],\n",
    "                'AnswerDText': mcq['AnswerDText'],\n",
    "                'MisconceptionDName': mcq['MisconceptionDName'],\n",
    "                'ReasoningD': mcq['ReasoningD'],\n",
    "                'CorrectAnswer': mcq['CorrectAnswer'],\n",
    "                'CorrectAnswerReasoning': mcq['CorrectAnswerReasoning']\n",
    "            }\n",
    "            syn_df_list.append(pd.DataFrame([row]))\n",
    "    except:\n",
    "        errer_res_list.append(res)\n",
    "    # break\n",
    "syn_dfs = pd.concat(syn_df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_dfs.to_excel('syn_data_4.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mids = str(int(evaluation_misconceptions_df.loc[8,'MisconceptionId'].item()))\n",
    "rids = missing_similar_misconceptions[mids]\n",
    "res = get_res([mids]+rids,df)\n",
    "reference_analysis, mcq_data = res['reference_analysis'], res['MCQ_generation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T15:45:13.652283Z",
     "iopub.status.busy": "2024-12-28T15:45:13.651818Z",
     "iopub.status.idle": "2024-12-28T15:45:13.669479Z",
     "shell.execute_reply": "2024-12-28T15:45:13.668165Z",
     "shell.execute_reply.started": "2024-12-28T15:45:13.652247Z"
    }
   },
   "source": [
    "## 6\\ Large-Scale Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # debug\n",
    "# rows = []\n",
    "# i = 0\n",
    "# mids = missing_ids[BatchSize*i:BatchSize*(i+1)] if BatchSize*(i+1)<=len(missing_ids) else len(missing_ids)\n",
    "# rids = misconception_ids[BatchSize*i:BatchSize*(i+1)]\n",
    "# for i in mids:\n",
    "#     rows.append('Misconception:' +content_df.loc[int(i),'MisconceptionName'])\n",
    "# cluster_misconceptions = '\\n'.join(rows)\n",
    "# reference_mcqs = gen_reference_mcq(rids,df) #input\n",
    "# num_mcqs = len(mids) # generate 2 questions for each misconception\n",
    "\n",
    "# prompt = f\"\"\"You are a Mathematics teacher. Your task is to generate Multiple Choice Questions (MCQs) that diagnose the following misconceptions:\n",
    "# <misconceptions>\n",
    "# {cluster_misconceptions}\n",
    "# </misconceptions>\n",
    "\n",
    "# Here are reference MCQs that demonstrate the questions and misconceptions:\n",
    "# <reference_mcqs>\n",
    "# {reference_mcqs}\n",
    "# </reference_mcqs>\n",
    "\n",
    "# First, analyze the reference MCQs carefully:\n",
    "# 1. Understand how to derive the correct answer and how the wrong answers map to respective misconceptions\n",
    "# 2. Note the style, difficulty level, and precision of language used\n",
    "# 3. List misconceptions still need coverage\n",
    "\n",
    "# Then, genrate {num_mcqs} new MCQs that diagnose misconceptions not already covered by the reference MCQs.\n",
    "# For each needed misconception, in your <reasoning> tag, show your process following these guidelines:\n",
    "# 1. brainstorm mathematical contexts where it commonly appears\n",
    "# 2. solve the probelm yourself and then design wrong answers leading to specific miesconceptions\n",
    "# 3. Make questions challenging enough that students must demonstrate real understanding\n",
    "# 4. ensure the answers are plausible and stem from genuine misconceptions, not careless errors\n",
    "# 5. Use precise mathematical language matching the style of reference MCQs\n",
    "# 5. Keep the construct name and subject name as short as possible\n",
    "\n",
    "# \"\"\"\n",
    "# response = client.chat.completions.create(\n",
    "# model=datagen_model,\n",
    "# messages=[\n",
    "#     {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to generate synthetic data.\"},\n",
    "#     {\"role\": \"user\", \"content\": prompt}\n",
    "#     ]\n",
    "# )\n",
    "# res = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7\\ LLM-as-judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROBLEM_DATA = str(syn_data_org.loc[2,:].to_json())\n",
    "prompt = f'''\n",
    "Here is the problem with both correct and incorrect answers. The suspected misconception is also provided:\n",
    "{PROBLEM_DATA}\n",
    "\n",
    "Analyze the problem and provide your evaluation:\n",
    "1. in \"CorrectSolution\", solve the problem independently to verify the correct answer\n",
    "2. in \"MisconceptionAReview\"/”MisconceptionBReview”/”MisconceptionCReview”/”MisconceptionDReview”, examine how someone holding the suspected misconception would \\\n",
    "approach the problem, trace the logical path from misconception to incorrect answer and identify any gaps or inconsistencies in this connection\n",
    "3. in \"MisconceptionAScore\"/”MisconceptionBScore”/”MisconceptionCScore”/”MisconceptionDScore”, score from 0-10 based on these criteria:\n",
    "   - 10: Perfect alignment - wrong answer is direct result of misconception\n",
    "   - 8-9: Strong alignment - clear logical path from misconception to answer\n",
    "   - 5-7: Moderate alignment - connection exists but has some gaps\n",
    "   - 1-4: Weak alignment - connection is unclear or requires assumptions\n",
    "   - 0: No alignment - misconception does not explain wrong answer\n",
    "   for correct answer which you think is correct, score 10, for correct answer which you think is incorrect, score 0\n",
    "Important guidelines:\n",
    "- Focus solely on the logical connection between misconception and wrong answer\n",
    "- Do not speculate about other possible misconceptions\n",
    "- Be specific about how the misconception leads to the error\n",
    "- Flag and deduct scores if any assumptions are required to connect misconception to answer\n",
    "- Consider whether a student with this misconception would consistently arrive at this wrong answer\n",
    "\n",
    "Output your evaluation results in the following json format:\n",
    "{{\n",
    "    \"CorrectSolution\": \"write down the correct solution process\",\n",
    "    \"CorrectAnswer\": \"write the correct answer(A/B/C/D)\",\n",
    "    \"MisconceptionAReview\": \"write down your evaluation though for MisconceptionA\",\n",
    "    \"MisconceptionAScore\": \"write down the score for MisconceptionA\",\n",
    "    \"MisconceptionBReview\": \"write down your evaluation though for MisconceptionB\",\n",
    "    \"MisconceptionBScore\": \"write down the score for MisconceptionB\",\n",
    "    \"MisconceptionCReview\": \"write down your evaluation though for MisconceptionC\",\n",
    "    \"MisconceptionCScore\": \"write down the score for MisconceptionC\",\n",
    "    \"MisconceptionDReview\": \"write down your evaluation though for MisconceptionD\",\n",
    "    \"MisconceptionDScore\": \"write down the score for MisconceptionD\",\n",
    "}}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=datagen_model,\n",
    "    temperature=0.1,\n",
    "    response_format={\"type\": \"json_object\"},\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You will analyze if the correct answer is actually correct and how how well an incorrect answer reflects a suspected misconception in a mathematics problem.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "      ]\n",
    "    )\n",
    "res = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9738540,
     "sourceId": 82695,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
