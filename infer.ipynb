{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":82695,"databundleVersionId":9738540,"sourceType":"competition"},{"sourceId":8218776,"sourceType":"datasetVersion","datasetId":4871830},{"sourceId":8897601,"sourceType":"datasetVersion","datasetId":5297895},{"sourceId":9760679,"sourceType":"datasetVersion","datasetId":5977063},{"sourceId":9818679,"sourceType":"datasetVersion","datasetId":6020055},{"sourceId":9901827,"sourceType":"datasetVersion","datasetId":6082713},{"sourceId":9901995,"sourceType":"datasetVersion","datasetId":6082859},{"sourceId":9939367,"sourceType":"datasetVersion","datasetId":6110821},{"sourceId":9948011,"sourceType":"datasetVersion","datasetId":6117312},{"sourceId":10006164,"sourceType":"datasetVersion","datasetId":6159548},{"sourceId":10093750,"sourceType":"datasetVersion","datasetId":6224448},{"sourceId":10094112,"sourceType":"datasetVersion","datasetId":6137029},{"sourceId":10111307,"sourceType":"datasetVersion","datasetId":4581967},{"sourceId":200567623,"sourceType":"kernelVersion"},{"sourceId":210716415,"sourceType":"kernelVersion"},{"sourceId":118192,"sourceType":"modelInstanceVersion","modelInstanceId":99392,"modelId":123481},{"sourceId":174909,"sourceType":"modelInstanceVersion","modelInstanceId":148911,"modelId":171421},{"sourceId":174921,"sourceType":"modelInstanceVersion","modelInstanceId":148923,"modelId":171434}],"dockerImageVersionId":30787,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install /kaggle/input/eedi-wheel/triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl --find-links /kaggle/input/eedi-wheel\n!pip install /kaggle/input/eedi-wheel/autoawq-0.2.7.post2-py3-none-any.whl --find-links /kaggle/input/eedi-wheel\n!pip install /kaggle/input/eedi-wheel/bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl --find-links /kaggle/input/eedi-wheel\n!pip install /kaggle/input/eedi-wheel/peft-0.13.2-py3-none-any.whl --find-links /kaggle/input/eedi-wheel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T09:41:22.435228Z","iopub.execute_input":"2024-12-05T09:41:22.435694Z","iopub.status.idle":"2024-12-05T09:42:50.00013Z","shell.execute_reply.started":"2024-12-05T09:41:22.435659Z","shell.execute_reply":"2024-12-05T09:42:49.999261Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#https://www.kaggle.com/datasets/wuwenmin/bge-large-en-v1-5\n#https://www.kaggle.com/datasets/syzong/qwen2-5-14b-instruct\n#https://www.kaggle.com/datasets/gmhost/qwen2-5-32b-instruct-quant\n#https://www.kaggle.com/datasets/abdurrafae/vllm-t4-fix\n#https://www.kaggle.com/datasets/eugenkrylov/vllm-0-6-3-post1-wheels\n#https://www.kaggle.com/datasets/emiz6413/lmsys-wheel-files\n#https://www.kaggle.com/datasets/nbroad/hf-libraries\n#https://www.kaggle.com/models/anhvth226/2211-lora-14b/Transformers/default/1\n#https://www.kaggle.com/models/anhvth226/qw14b-awq/Transformers/default/1\n#https://www.kaggle.com/code/ironbar/making-wheels-of-necessary-packages-for-vllm\n#https://www.kaggle.com/models/takanashihumbert/qwen2.5/Transformers/32b-instruct-awq/1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 检索模型\n     1.开源模型-Qwen-14b-AWQ的微调\n     2.Qwen-14b-instruct的量化微调\n     3.Qwen-32b-instruct的量化微调","metadata":{}},{"cell_type":"code","source":"%%writefile run_embed.py\nimport argparse\nimport os\nimport json\nimport torch\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom transformers import AutoTokenizer, AutoModel\nfrom tqdm import tqdm\nimport peft\n\nMAX_LENGTH = 384\n\n\ndef last_token_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n    left_padding = attention_mask[:, -1].sum() == attention_mask.shape[0]\n    if left_padding:\n        return last_hidden_states[:, -1]\n    else:\n        sequence_lengths = attention_mask.sum(dim=1) - 1\n        batch_size = last_hidden_states.shape[0]\n        return last_hidden_states[\n            torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths\n        ]\n\n\ndef get_embeddings_in_batches(model, tokenizer, texts, max_length, batch_size=32):\n    embeddings = []\n    for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding\"):\n        batch_texts = texts[i : i + batch_size]\n        batch_dict = tokenizer(\n            batch_texts,\n            max_length=max_length,\n            padding=True,\n            truncation=True,\n            return_tensors=\"pt\",\n        ).to(\"cuda\")\n        with torch.no_grad(), torch.amp.autocast(\"cuda\"):\n            outputs = model(**batch_dict)\n            batch_embeddings = last_token_pool(\n                outputs.last_hidden_state, batch_dict[\"attention_mask\"]\n            )\n            batch_embeddings = F.normalize(batch_embeddings, p=2, dim=1).cpu()\n        embeddings.append(batch_embeddings)\n    return torch.cat(embeddings, dim=0)\n\n\ndef load_model_and_tokenizer(base_model_path, lora_path, load_in_4bit=True):\n    model = AutoModel.from_pretrained(\n        base_model_path,\n        device_map=0,\n        torch_dtype=torch.float16,\n        load_in_4bit=load_in_4bit,\n    )\n    tokenizer = AutoTokenizer.from_pretrained(\n        lora_path if lora_path else base_model_path\n    )\n    model.resize_token_embeddings(len(tokenizer))\n    if lora_path:\n        model = peft.PeftModel.from_pretrained(model, lora_path)\n    return model, tokenizer\n\n\ndef main(args):\n    output_file = args.input_text.replace(\n        \".json\", \".pt.fold.{}.{}.embed\".format(*args.fold)\n    )\n    if os.path.exists(output_file):\n        print(f\"Output file {output_file} already exists. Skipping...\")\n        return\n    model, tokenizer = load_model_and_tokenizer(\n        args.base_model, args.lora_path, load_in_4bit=args.load_in_4bit\n    )\n    texts = json.load(open(args.input_text))[\"texts\"][args.fold[0] :: args.fold[1]]\n    embeddings = get_embeddings_in_batches(\n        model,\n        tokenizer,\n        texts,\n        max_length=MAX_LENGTH,\n        batch_size=4,\n    )\n    text2embeds = {text: emb for text, emb in zip(texts, embeddings)}\n    torch.save(text2embeds, output_file)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--base_model\",\n        type=str,\n        default=\"Qwen/Qwen2.5-7B\",\n        help=\"Path to the base model\",\n    )\n    parser.add_argument(\n        \"--lora_path\",\n        type=str,\n        default=None,\n        help=\"Path to the LoRA model\",\n    )\n    parser.add_argument(\n        \"--input_text\",\n        type=str,\n        default=\".cache/data.json\",\n    )\n    parser.add_argument(\n        \"--load_in_4bit\",\n        action=\"store_true\",\n        help=\"Load model in 4-bit mode\",\n    )\n    parser.add_argument(\"--fold\", nargs=2, type=int, default=[0, 1])\n    args = parser.parse_args()\n    if not os.path.exists(args.lora_path):\n        args.lora_path = None\n    main(args)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T09:44:14.420861Z","iopub.execute_input":"2024-12-05T09:44:14.421176Z","iopub.status.idle":"2024-12-05T09:44:14.42889Z","shell.execute_reply.started":"2024-12-05T09:44:14.421146Z","shell.execute_reply":"2024-12-05T09:44:14.427966Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile qwen14b_awq_infer.py\nimport os, math, numpy as np\nimport sys\nimport os\nfrom transformers import AutoTokenizer\nimport pandas as pd\nfrom tqdm import tqdm\nimport re, gc\nimport torch\npd.set_option('display.max_rows', 300)\n\nIS_SUBMISSION = True\nprint('IS_SUBMISSION:', IS_SUBMISSION)\ndf_train = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/train.csv\").fillna(-1).sample(10, random_state=42).reset_index(drop=True)\ndf_test = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/test.csv\")\ndf_misconception_mapping = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\")\nif len(df_test)<10:\n    df_misconception_mapping = df_misconception_mapping.head(25)\n\nimport pandas as pd\nif not IS_SUBMISSION:\n    df_ret = df_train.copy()\nelse:\n    df_ret = df_test.copy()\n\nTEMPLATE_INPUT_V3 = '{QUESTION}\\nCorrect answer: {CORRECT_ANSWER}\\nStudent wrong answer: {STUDENT_WRONG_ANSWER}'\ndef format_input_v3(row, wrong_choice):\n\n    assert wrong_choice in \"ABCD\"\n    # Extract values from the row\n    question_text = row.get(\"QuestionText\", \"No question text provided\")\n    subject_name = row.get(\"SubjectName\", \"Unknown subject\")\n    construct_name = row.get(\"ConstructName\", \"Unknown construct\")\n    # Extract the correct and wrong answer text based on the choice\n    correct_answer = row.get(\"CorrectAnswer\", \"Unknown\")\n    assert wrong_choice != correct_answer\n    correct_answer_text = row.get(f\"Answer{correct_answer}Text\", \"No correct answer text available\")\n    wrong_answer_text = row.get(f\"Answer{wrong_choice}Text\", \"No wrong answer text available\")\n\n    # Construct the question format\n    formatted_question = f\"\"\"Question: {question_text}\n    \nSubjectName: {subject_name}\nConstructName: {construct_name}\"\"\"\n\n    # Return the extracted data\n    ret = {\n        \"QUESTION\": formatted_question,\n        \"CORRECT_ANSWER\": correct_answer_text,\n        \"STUDENT_WRONG_ANSWER\": wrong_answer_text,\n        \"MISCONCEPTION_ID\": row.get('Misconception{wrong_choice}Id'),\n    }\n    ret[\"PROMPT\"] = TEMPLATE_INPUT_V3.format(**ret)\n\n    return ret\n\n\nitems = []\ntarget_ids = []\nfor _, row in df_ret.iterrows():\n    for choice in ['A', 'B', 'C', 'D']:\n        if choice == row[\"CorrectAnswer\"]:\n            continue\n        if not IS_SUBMISSION and row[f'Misconception{choice}Id'] == -1:\n            continue\n            \n        correct_col = f\"Answer{row['CorrectAnswer']}Text\"\n        item = {'QuestionId_Answer': '{}_{}'.format(row['QuestionId'], choice)}\n        item['Prompt'] = format_input_v3(row, choice)['PROMPT']\n        items.append(item)\n        target_ids.append(int(row.get(f'Misconception{choice}Id', -1)))\n        \ndf_input = pd.DataFrame(items)\n\ndef get_detailed_instruct(task_description: str, query: str) -> str:\n    return f'<instruct>{task_description}\\n<query>{query}'\n\ndef get_detailed_example(task_description: str, query: str, response: str) -> str:\n    return f'<instruct>{task_description}\\n<query>{query}\\n<response>{response}'\n\ndef get_new_queries(queries, query_max_len, examples_prefix, tokenizer):\n    inputs = tokenizer(\n        queries,\n        max_length=query_max_len - len(tokenizer('<s>', add_special_tokens=False)['input_ids']) - len(\n            tokenizer('\\n<response></s>', add_special_tokens=False)['input_ids']),\n        return_token_type_ids=False,\n        truncation=True,\n        return_tensors=None,\n        add_special_tokens=False\n    )\n    prefix_ids = tokenizer(examples_prefix, add_special_tokens=False)['input_ids']\n    suffix_ids = tokenizer('\\n<response>', add_special_tokens=False)['input_ids']\n    new_max_length = (len(prefix_ids) + len(suffix_ids) + query_max_len + 8) // 8 * 8 + 8\n    new_queries = tokenizer.batch_decode(inputs['input_ids'])\n    for i in range(len(new_queries)):\n        new_queries[i] = examples_prefix + new_queries[i] + '\\n<response>'\n    return new_max_length, new_queries\ntask =  \"Given a math multiple-choice problem with a student's wrong answer, retrieve the math misconceptions\"\nqueries = [\n    get_detailed_instruct(task, q) for q in df_input['Prompt']\n]\ndocuments = df_misconception_mapping['MisconceptionName'].tolist()\nquery_max_len, doc_max_len = 420, 48\nLORA_PATH = '/kaggle/input/2211-lora-14b/transformers/default/1'\ntokenizer = AutoTokenizer.from_pretrained(LORA_PATH)\nexamples_prefix = ''\nnew_query_max_len, new_queries = get_new_queries(queries, query_max_len, examples_prefix, tokenizer)\n\n\nimport json\nwith open('data.json', 'w') as f:\n    data = {'texts': new_queries+ documents}\n    f.write(json.dumps(data))\n\nlora_path = '/kaggle/input/2211-lora-14b/transformers/default/1'\ncmd = f\"(CUDA_VISIBLE_DEVICES=0 python run_embed.py --base_model /kaggle/input/qw14b-awq/transformers/default/1 --lora_path {lora_path} --input_text data.json --fold 0 2) & (CUDA_VISIBLE_DEVICES=1 python run_embed.py --base_model /kaggle/input/qw14b-awq/transformers/default/1 --lora_path {lora_path} --input_text data.json --fold 1 2)\"\nimport os\nos.system(cmd)\n\nfrom glob import glob\nimport time\ntext_to_embed = {}\nfiles = glob('*.pt*')\nwhile len(files) != 2:\n    time.sleep(1)\n    files = glob('*.pt*')\n\n\ntime.sleep(3)    \nfor path in files:\n    print(path)\n    text_to_embed.update(torch.load(path))\n\n\nquery_embeddings = torch.stack([text_to_embed[t] for t in new_queries])\ndoc_embeddings = torch.stack([text_to_embed[t] for t in documents])\nquery_embeddings.shape, doc_embeddings.shape\nquery_embeddings = query_embeddings.numpy()\ndoc_embeddings = doc_embeddings.numpy()\nnp.save(\"qwen14b_awq_query_embeddings.npy\", query_embeddings)\nnp.save(\"qwen14b_awq_doc_embeddings.npy\", doc_embeddings)","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T09:44:14.430018Z","iopub.execute_input":"2024-12-05T09:44:14.430235Z","iopub.status.idle":"2024-12-05T09:44:14.448303Z","shell.execute_reply.started":"2024-12-05T09:44:14.430213Z","shell.execute_reply":"2024-12-05T09:44:14.447429Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python qwen14b_awq_infer.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T09:44:14.449376Z","iopub.execute_input":"2024-12-05T09:44:14.449627Z","iopub.status.idle":"2024-12-05T09:47:23.010839Z","shell.execute_reply.started":"2024-12-05T09:44:14.449604Z","shell.execute_reply":"2024-12-05T09:47:23.009725Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile qwen14b_inst.py\nimport ctypes\nimport gc\nimport torch\ndef clean_memory(deep=True):\n    gc.collect()\n    if deep:\n        ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n    torch.cuda.empty_cache()\nclean_memory()\nfrom tqdm.auto import tqdm\nfrom bs4 import BeautifulSoup\nimport gc\nimport pandas as pd\nimport pickle\nimport sys\nimport numpy as np\nfrom tqdm.auto import trange\nfrom sklearn.model_selection import GroupKFold\nimport json\nimport torch\nimport torch.nn as nn\nfrom numpy.linalg import norm\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom transformers import AutoTokenizer,AutoConfig,AutoModel,BitsAndBytesConfig\nfrom peft import (\n    LoraConfig,\n    get_peft_model,\n    PeftModel,\n)\nimport json\nimport copy\nimport warnings\nimport re\nwarnings.filterwarnings('ignore')\nfrom concurrent.futures import ThreadPoolExecutor\nfrom sklearn.neighbors import NearestNeighbors\n\npath_prefix = \"/kaggle/input/eedi-mining-misconceptions-in-mathematics\"\nmodel_path = \"/kaggle/input/qwen2-5-14b-instruct\"\nlora_path=\"/kaggle/input/eedi-qwen-lora/qwen_v10_last/output-qwen_v10/last\"\ndevice_0 = torch.device('cuda:0')\ndevice_1 = torch.device('cuda:1')\nq_max_len = 512\np_max_len = 50\ntop_k = 25\n\n\n# %%\ndef batch_to_device(batch, target_device):\n    \"\"\"\n    send a pytorch batch to a device (CPU/GPU)\n    \"\"\"\n    for key in batch:\n        if isinstance(batch[key], Tensor):\n            batch[key] = batch[key].to(target_device)\n    return batch\n\ndef last_token_pool(last_hidden_states: Tensor,\n                    attention_mask: Tensor) -> Tensor:\n    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n    if left_padding:\n        return last_hidden_states[:, -1]\n    else:\n        sequence_lengths = attention_mask.sum(dim=1) - 1\n        batch_size = last_hidden_states.shape[0]\n        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n\ndef get_detailed_instruct(task_description: str, query: str) -> str:\n    return f'Instruct: {task_description}\\nQuery: {query}'\n\n# %%\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    #llm_int8_skip_modules=[\"proj_head\"]\n)\n\n# %%\nmodel_config = AutoConfig.from_pretrained(model_path)\n\n# %%\ntokenizer = AutoTokenizer.from_pretrained(model_path)\ntokenizer.truncation_side = \"left\"\n\n# %%\nclass CustomSimCSEModel(nn.Module):\n    def __init__(self, path, config, device, quantization_config, top_linear=True, emb_size=1024, sentence_pooling_method='last', normlized=True, temperature=0.02):\n        super().__init__()\n        self.model = AutoModel.from_pretrained(path, config=config, quantization_config=quantization_config, trust_remote_code=True, device_map=device)\n        self.config = self.model.config\n        self.top_linear = top_linear\n        if self.top_linear:\n            self.proj_head = nn.Linear(config.hidden_size, emb_size)\n            self.proj_head.to(device)\n        self.sentence_pooling_method = sentence_pooling_method\n        self.normlized = normlized\n        self.temperature = temperature\n        self.cross_entropy = nn.CrossEntropyLoss(reduction='mean')\n        \n    def last_token_pool(self, last_hidden_states: Tensor,\n                        attention_mask: Tensor) -> Tensor:\n        left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n        if left_padding:\n            return last_hidden_states[:, -1]\n        else:\n            sequence_lengths = attention_mask.sum(dim=1) - 1\n            batch_size = last_hidden_states.shape[0]\n            return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n        \n    def sentence_embedding(self, hidden_state, mask):\n        if self.sentence_pooling_method == 'mean':\n            s = torch.sum(hidden_state * mask.unsqueeze(-1).float(), dim=1)\n            d = mask.sum(axis=1, keepdim=True).float()\n            return s / d\n        elif self.sentence_pooling_method == 'cls':\n            return hidden_state[:, 0]\n        elif self.sentence_pooling_method == 'last':\n            return self.last_token_pool(hidden_state, mask)\n        \n    def encode(self, features):\n        if features is None:\n            return None\n        psg_out = self.model(input_ids=features['input_ids'], attention_mask=features['attention_mask'],\n                             return_dict=True)\n        p_reps = self.sentence_embedding(psg_out.last_hidden_state, features['attention_mask'])\n        p_reps = p_reps.to(torch.float32)\n        if self.top_linear:\n            p_reps = self.proj_head(p_reps)\n        if self.normlized:\n            p_reps = torch.nn.functional.normalize(p_reps, dim=-1)\n        return p_reps.contiguous()\n    \n    def compute_similarity(self, q_reps, p_reps):\n        if len(p_reps.size()) == 2:\n            return torch.matmul(q_reps, p_reps.transpose(0, 1))\n        return torch.matmul(q_reps, p_reps.transpose(-2, -1))\n    \n    def forward(self, query, doc):\n        query_emb = self.encode(query)\n        doc_emb = self.encode(doc)\n        scores = self.compute_similarity(query_emb, doc_emb) / self.temperature\n        scores = scores.view(query_emb.size(0), -1)\n        target = torch.arange(scores.size(0), device=scores.device, dtype=torch.long)\n        loss = self.cross_entropy(scores, target)\n        return dict(\n            loss=loss,\n            scores=scores,\n            query_emb=query_emb,\n            doc_emb=doc_emb,\n        )\n\n\n# %%\ntask_description = 'Given a math question and a misconcepte incorrect answer, please retrieve the most accurate reason for the misconception.'\n\n# %%\n\ntra = pd.read_csv(f\"{path_prefix}/test.csv\")\nprint(tra.shape)\nmisconception_mapping = pd.read_csv(f\"{path_prefix}/misconception_mapping.csv\")\nif tra.shape[0]<10:\n    misconception_mapping = misconception_mapping.head(25)\n\n\n# %%\ndef create_train_df(train_df, misconception_mapping, is_train=True):\n    train_data = []\n    for _,row in train_df.iterrows():\n        for c in ['A','B','C','D']:\n            if is_train:\n                misconception_id = row[f\"Misconception{c}Id\"]\n                if np.isnan(misconception_id):\n                    continue\n                misconception_id = int(misconception_id)\n            if c == row['CorrectAnswer']:\n                continue\n            if f'Answer{c}Text' not in row:\n                continue\n            real_answer_id = row['CorrectAnswer']\n            real_text = row[f'Answer{real_answer_id}Text']\n            incorrect_text = row[f'Answer{c}Text']\n            query_text =f\"###question###:{row['SubjectName']}-{row['ConstructName']}-{row['QuestionText']}\\n###Correct Answer###:{real_text}\\n###Misconcepte Incorrect answer###:{incorrect_text}\"\n            row['CorrectAnswerText'] = real_text\n            row['IncorrectAnswerText'] = incorrect_text\n            row['query'] = get_detailed_instruct(task_description,query_text)\n            row['answer_name'] = c\n            if is_train:\n                row['answer_id'] = misconception_id\n                row['doc'] = misconception_mapping.iloc[misconception_id]['MisconceptionName']\n            train_data.append(copy.deepcopy(row))\n    new_train_df = pd.DataFrame(train_data)\n    return new_train_df\n\n# %%\nnew_val_df = create_train_df(tra, misconception_mapping, is_train=False)\n\n# %%\ndef inference(df, model, tokenizer, max_length, device):\n    batch_size = 8\n    sentences = list(df['query'].values)\n    all_embeddings = []\n    length_sorted_idx = np.argsort([-len(sen) for sen in sentences])\n    sentences_sorted = [sentences[idx] for idx in length_sorted_idx]\n    for start_index in trange(0, len(sentences), batch_size, desc=\"Batches\", disable=False):\n        sentences_batch = sentences_sorted[start_index: start_index + batch_size]\n        features = tokenizer(sentences_batch, max_length=max_length, padding=True, truncation=True,\n                             return_tensors=\"pt\")\n        features = batch_to_device(features, device)\n        with torch.no_grad():\n            embeddings = model.encode(features)\n            embeddings = embeddings.detach().cpu().numpy().tolist()\n        all_embeddings.extend(embeddings)\n\n    all_embeddings = [np.array(all_embeddings[idx]).reshape(1, -1) for idx in np.argsort(length_sorted_idx)]\n\n    sentence_embeddings = np.concatenate(all_embeddings, axis=0)\n    return sentence_embeddings\n\n# %%\nmodel_0 = CustomSimCSEModel(model_path, config=model_config, quantization_config=bnb_config, top_linear=False, device=device_0)\nmodel_0 = PeftModel.from_pretrained(model_0, lora_path, device_map=device_0)\n\n\nmodel_1 = CustomSimCSEModel(model_path, config=model_config, quantization_config=bnb_config, top_linear=False, device=device_1)\nmodel_1 = PeftModel.from_pretrained(model_1, lora_path, device_map=device_1)\n# %%\n\nif torch.cuda.device_count() > 1:\n    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n\n\n# %%\nmid = len(new_val_df) // 2\nsub_1 = new_val_df.iloc[:mid].copy()\nsub_2 = new_val_df.iloc[mid:].copy()\n\nwith ThreadPoolExecutor(max_workers=2) as executor:\n    q_results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (tokenizer, tokenizer), (q_max_len, q_max_len), (device_0, device_1))\n\nquery_embeddings = np.concatenate(list(q_results), axis=0)\nnp.save(\"qwen14b_inst_query_embeddings.npy\", query_embeddings)\nprint(\"query_embeddings:\")\nprint(query_embeddings.shape)\nmisconception_mapping['query'] = misconception_mapping['MisconceptionName']\n\nmmid = len(misconception_mapping) // 2\nmsub_1 = misconception_mapping.iloc[:mmid].copy()\nmsub_2 = misconception_mapping.iloc[mmid:].copy()\n\nwith ThreadPoolExecutor(max_workers=2) as executor:\n    d_results = executor.map(inference, (msub_1, msub_2), (model_0, model_1), (tokenizer, tokenizer), (p_max_len, p_max_len), (device_0, device_1))\n\ndoc_embeddings = np.concatenate(list(d_results), axis=0)\nnp.save(\"qwen14b_inst_doc_embeddings.npy\", doc_embeddings)\nprint(\"doc_embeddings:\")\nprint(doc_embeddings.shape)\n#indices = get_matches(query_embeddings, doc_embeddings, n_neighbors=top_k)\nnew_val_df[\"QuestionId_Answer\"] = new_val_df[\"QuestionId\"].astype(str) + \"_\" + new_val_df[\"answer_name\"]\nnew_val_df.to_parquet('df.parquet', index=False)\nprint(\"Recall data file created successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T09:47:28.679145Z","iopub.execute_input":"2024-12-05T09:47:28.679485Z","iopub.status.idle":"2024-12-05T09:47:28.689948Z","shell.execute_reply.started":"2024-12-05T09:47:28.679455Z","shell.execute_reply":"2024-12-05T09:47:28.689113Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python qwen14b_inst.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T09:47:31.926285Z","iopub.execute_input":"2024-12-05T09:47:31.92695Z","iopub.status.idle":"2024-12-05T09:49:59.700388Z","shell.execute_reply.started":"2024-12-05T09:47:31.926913Z","shell.execute_reply":"2024-12-05T09:49:59.699554Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile qwen32b_infer.py\nimport ctypes\nimport gc\nimport torch\ndef clean_memory(deep=True):\n    gc.collect()\n    if deep:\n        ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n    torch.cuda.empty_cache()\nclean_memory()\nfrom tqdm.auto import tqdm\nfrom bs4 import BeautifulSoup\nimport gc\nimport pandas as pd\nimport pickle\nimport sys\nimport numpy as np\nfrom tqdm.auto import trange\nfrom sklearn.model_selection import GroupKFold\nimport json\nimport torch\nimport torch.nn as nn\nfrom numpy.linalg import norm\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom transformers import AutoTokenizer,AutoConfig,AutoModel,BitsAndBytesConfig\nfrom peft import (\n    LoraConfig,\n    get_peft_model,\n    PeftModel,\n)\nimport json\nimport copy\nimport warnings\nimport re\nwarnings.filterwarnings('ignore')\nfrom concurrent.futures import ThreadPoolExecutor\nfrom sklearn.neighbors import NearestNeighbors\n\n\n\ndef batch_to_device(batch, target_device):\n    \"\"\"\n    send a pytorch batch to a device (CPU/GPU)\n    \"\"\"\n    for key in batch:\n        if isinstance(batch[key], Tensor):\n            batch[key] = batch[key].to(target_device)\n    return batch\n\ndef last_token_pool(last_hidden_states: Tensor,\n                    attention_mask: Tensor) -> Tensor:\n    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n    if left_padding:\n        return last_hidden_states[:, -1]\n    else:\n        sequence_lengths = attention_mask.sum(dim=1) - 1\n        batch_size = last_hidden_states.shape[0]\n        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n\ndef get_detailed_instruct(task_description: str, query: str) -> str:\n    return f'Instruct: {task_description}\\nQuery: {query}'\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n)\n\npath_prefix = \"/kaggle/input/eedi-mining-misconceptions-in-mathematics\"\nmodel_path = \"/kaggle/input/qwen2-5-32b-instruct-quant\"\nlora_path=\"/kaggle/input/qwen2-5-32b-n6-convert/pp_adapter-n6-last/\"\nVALID = False\nq_max_len = 384\np_max_len = 50\ntop_k = 25\nmodel_config = AutoConfig.from_pretrained(model_path)\ntokenizer = AutoTokenizer.from_pretrained(model_path)\ntokenizer.truncation_side = \"left\"\nmodel = AutoModel.from_pretrained(model_path, config=model_config, \n                                  quantization_config=bnb_config, \n                                  trust_remote_code=True, device_map=\"auto\", torch_dtype=torch.bfloat16)\n\nmodel = PeftModel.from_pretrained(model, lora_path)\nmodel.config.use_cache = False\n\ntask_description = 'Given a math question and a misconcepte incorrect answer, please retrieve the most accurate reason for the misconception.'\n\ntra = pd.read_csv(f\"{path_prefix}/test.csv\")\nprint(tra.shape)\nmisconception_mapping = pd.read_csv(f\"{path_prefix}/misconception_mapping.csv\")\nif tra.shape[0]<10:\n    misconception_mapping = misconception_mapping.head(25)\n\ndef create_train_df(train_df, misconception_mapping, is_train=True):\n    train_data = []\n    for _,row in train_df.iterrows():\n        for c in ['A','B','C','D']:\n            if is_train:\n                misconception_id = row[f\"Misconception{c}Id\"]\n                if np.isnan(misconception_id):\n                    continue\n                misconception_id = int(misconception_id)\n            if c == row['CorrectAnswer']:\n                continue\n            if f'Answer{c}Text' not in row:\n                continue\n            real_answer_id = row['CorrectAnswer']\n            real_text = row[f'Answer{real_answer_id}Text']\n            incorrect_text = row[f'Answer{c}Text']\n            query_text =f\"###question###:{row['SubjectName']}-{row['ConstructName']}-{row['QuestionText']}\\n###Correct Answer###:{real_text}\\n###Misconcepte Incorrect answer###:{incorrect_text}\"\n            row['CorrectAnswerText'] = real_text\n            row['IncorrectAnswerText'] = incorrect_text\n            row['query'] = get_detailed_instruct(task_description,query_text)\n            row['answer_name'] = c\n            if is_train:\n                row['answer_id'] = misconception_id\n                row['doc'] = misconception_mapping.iloc[misconception_id]['MisconceptionName']\n            train_data.append(copy.deepcopy(row))\n    new_train_df = pd.DataFrame(train_data)\n    return new_train_df\n\nnew_val_df = create_train_df(tra, misconception_mapping, is_train=VALID)\nmodel = model.eval()\ndef inference(df, model, tokenizer, max_length):\n    batch_size = 4\n    sentences = list(df['query'].values)\n    all_embeddings = []\n    length_sorted_idx = np.argsort([-len(sen) for sen in sentences])\n    sentences_sorted = [sentences[idx] for idx in length_sorted_idx]\n    for start_index in trange(0, len(sentences), batch_size, desc=\"Batches\", disable=False):\n        sentences_batch = sentences_sorted[start_index: start_index + batch_size]\n        features = tokenizer(sentences_batch, max_length=max_length, padding=True, truncation=True,\n                             return_tensors=\"pt\")\n        features = batch_to_device(features, 'cuda')\n        with torch.no_grad():\n            output = model(**features)\n            last_token_rep = last_token_pool(output.last_hidden_state, features['attention_mask'])\n            rep = torch.nn.functional.normalize(last_token_rep.to(torch.float32), dim=-1)\n            rep = rep.detach().cpu().numpy().tolist()\n        all_embeddings.extend(rep)\n    all_embeddings = [np.array(all_embeddings[idx]).reshape(1, -1) for idx in np.argsort(length_sorted_idx)]\n    sentence_embeddings = np.concatenate(all_embeddings, axis=0)\n    return sentence_embeddings\n\nquery_embeddings = inference(new_val_df, model, tokenizer, q_max_len)\nmisconception_mapping['query'] = misconception_mapping['MisconceptionName']\ndoc_embeddings = inference(misconception_mapping, model, tokenizer, p_max_len)\n\nnp.save(\"qwen32b_query_embeddings.npy\", query_embeddings)\nprint(\"query_embeddings:\")\nprint(query_embeddings.shape)\nprint(query_embeddings)\n\nnp.save(\"qwen32b_doc_embeddings.npy\", doc_embeddings)\nprint(\"doc_embeddings:\")\nprint(doc_embeddings.shape)\nnew_val_df[\"QuestionId_Answer\"] = new_val_df[\"QuestionId\"].astype(str) + \"_\" + new_val_df[\"answer_name\"]\nnew_val_df.to_parquet('df.parquet', index=False)\nprint(\"Recall data file created successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T09:49:59.712161Z","iopub.execute_input":"2024-12-05T09:49:59.712396Z","iopub.status.idle":"2024-12-05T09:49:59.733047Z","shell.execute_reply.started":"2024-12-05T09:49:59.712373Z","shell.execute_reply":"2024-12-05T09:49:59.732215Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python qwen32b_infer.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T09:49:59.734407Z","iopub.execute_input":"2024-12-05T09:49:59.734722Z","iopub.status.idle":"2024-12-05T09:49:59.746032Z","shell.execute_reply.started":"2024-12-05T09:49:59.734697Z","shell.execute_reply":"2024-12-05T09:49:59.745352Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### embedding融合","metadata":{}},{"cell_type":"code","source":"%%writefile retrive_model.py\nimport ctypes\nimport gc\nimport torch\ndef clean_memory(deep=True):\n    gc.collect()\n    if deep:\n        ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n    torch.cuda.empty_cache()\nclean_memory()\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.neighbors import NearestNeighbors\n\ntop_k = 25\npath_prefix = \"/kaggle/input/eedi-mining-misconceptions-in-mathematics\"\n\ndef get_matches(V_topic, V_content, n_neighbors=25):\n    \n    neighbors_model = NearestNeighbors(n_neighbors=n_neighbors, metric='cosine', algorithm=\"brute\", n_jobs=-1)\n    neighbors_model.fit(V_content)\n    dists, indices = neighbors_model.kneighbors(V_topic)\n    \n    return indices\n\nnew_val_df = pd.read_parquet(\"df.parquet\")\nq_emb1 = np.load('/kaggle/working/qwen14b_awq_query_embeddings.npy').astype(\"float16\")\nq_emb2 = np.load('/kaggle/working/qwen32b_query_embeddings.npy').astype(\"float16\")\nq_emb3 = np.load('/kaggle/working/qwen14b_inst_query_embeddings.npy').astype(\"float16\")\n\nd_emb1 = np.load('/kaggle/working/qwen14b_awq_doc_embeddings.npy').astype(\"float16\")\nd_emb2 = np.load('/kaggle/working/qwen32b_doc_embeddings.npy').astype(\"float16\")\nd_emb3 = np.load('/kaggle/working/qwen14b_inst_doc_embeddings.npy').astype(\"float16\")\n\nquery_embeddings = np.concatenate([q_emb1,q_emb2, q_emb3], axis=1)\n\ndoc_embeddings = np.concatenate([d_emb1, d_emb2, d_emb3], axis=1)\n\ndoc_embeddings.shape\n\ntop_k = min(top_k, len(doc_embeddings))\nindices = get_matches(query_embeddings, doc_embeddings, n_neighbors=top_k)\nmisconception_mapping = pd.read_csv(f\"{path_prefix}/misconception_mapping.csv\")\nnp.save(\"indices.npy\", indices)\nnew_val_df['MisconceptionId_topk'] = indices.tolist()\nnew_val_df.to_parquet('submission1.parquet', index=False)\n# new_val_df[\"MisconceptionId\"] = new_val_df[\"recall_ids\"].apply(lambda x: \" \".join(map(str, x)))\n# new_val_df.to_csv(\"submission.csv\", columns=[\"QuestionId_Answer\", \"MisconceptionId\"], index=False)\n# print(\"Submission file created successfully!\")\n\n# new_val_df[['QuestionId_Answer', \"MisconceptionId\"]]\nprint(new_val_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T09:53:13.01295Z","iopub.execute_input":"2024-12-05T09:53:13.013298Z","iopub.status.idle":"2024-12-05T09:53:14.634933Z","shell.execute_reply.started":"2024-12-05T09:53:13.013269Z","shell.execute_reply":"2024-12-05T09:53:14.634256Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python retrive_model.py","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 重排\n    1.利用Qwen2.5-32B-Instruct-AWQ的zero-shot能力输出预测的语句\n    2.利用bge-large-en-v1.5检索最相似的候选","metadata":{}},{"cell_type":"code","source":"!pip install --no-index --find-links=/kaggle/input/vllm-0-6-3-post1-wheels torchvision==0.19.1\n!pip install --no-index --find-links=/kaggle/input/vllm-0-6-3-post1-wheels vllm\n!pip install --no-deps --no-index /kaggle/input/hf-libraries/sentence-transformers/sentence_transformers-3.1.0-py3-none-any.whl","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile run_vllm.py\n\nimport pandas as pd\nimport numpy as np\nimport vllm\nimport re\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom transformers import AutoTokenizer\nimport argparse\n\nap = argparse.ArgumentParser()\nap.add_argument('--GROUP', type=int, default=3, required=False)\nap.add_argument('--NUM_1', type=int, default=7,required=False)\nap.add_argument('--NUM_2', type=int, default=5,required=False)\nargs = ap.parse_args()\n\ntest_long = pd.read_parquet('submission1.parquet')\ntest_sorted_indices_topk = test_long['MisconceptionId_topk'].tolist()\ntest_sorted_indices_topk = [item.tolist() for item in test_sorted_indices_topk]\n\nMisconception_df = pd.read_csv('/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv') # 修改Misconception_df名字\nMisconception_id2name = dict(zip(Misconception_df['MisconceptionId'], Misconception_df['MisconceptionName']))\nMisconceptionName = Misconception_df['MisconceptionName'].tolist()\n\nllm_model_name = '/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1' # /kaggle/input/qwq-32b-preview-awq, /kaggle/input/qwen2-5-32b-instruct-awq\n\ndef apply_template(row, tokenizer):\n    PROMPT  = \"\"\"Here is a mathematics question about \nCurriculum knowledge: {constructName}({subjectName})\nQuestion: {problem}\nIncorrect Answer: {wrong_ans}\nCorrect Answer: {correctAnswerValue}\n    \nYou are a Mathematics teacher. Your task is to reason and identify the misconception behind the Incorrect Answer with the Question.\nAnswer concisely what misconception it is to lead to getting the incorrect answer.\nNo need to give the reasoning process and do not use \"The misconception is\" to start your answers.\nThere are some relative and possible misconceptions below to help you make the decision:\n\n{retrival}\"\"\"\n\n    messages = [\n      {\n          \"role\": \"user\", \n          \"content\": PROMPT.format(\n              constructName=row[\"ConstructName\"],\n              problem=row[\"QuestionText\"],\n              correctAnswerValue=row[\"CorrectAnswerText\"],\n              wrong_ans=row[\"IncorrectAnswerText\"],\n              subjectName=row[\"SubjectName\"],\n              retrival=row[\"Retrival\"]\n          )}\n  ]\n    \n    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    return text\n\ndef number2sentence(llm_output, test_sorted_indices_top, Misconception_id2name):\n    num = len(llm_output)\n    for i in range(num):\n        text = str(llm_output[i])\n        text = text.replace(\"<|im_start|>\", \"\")\n        text = text.replace(\"<|im_end|>\", \"\")\n        text = text.rstrip(\"\\n\").strip()\n        potential = re.search(r'^\\w+\\.{0,1}', text).group()\n        # print(potential)\n        if '.' in potential or re.fullmatch(r\"\\d+\", text):\n            if len(text) > 5:\n                sentence = text.replace(potential, '').strip()\n            else:\n                text = re.sub(r\"^(\\d+)\\.$\", r\"\\1\", text)\n                try:\n                    sentence = Misconception_id2name[test_sorted_indices_top[i][int(text) - 1]]\n                except:\n                    sentence = Misconception_id2name[test_sorted_indices_top[i][0]]\n        else:\n            sentence = text\n        llm_output[i] = sentence\n    return llm_output\n\nllm = vllm.LLM(\n        llm_model_name,\n        quantization=\"awq\",\n        tensor_parallel_size=2, \n        gpu_memory_utilization=0.95,  #qwen:0.96, mistrial:0.95, Mythalion: 0.9\n        trust_remote_code=False,\n        dtype=\"half\", \n        enforce_eager=True,\n        max_model_len=5120,\n        disable_log_stats=True,\n    )\n\ntokenizer = llm.get_tokenizer()\n\nfrom sentence_transformers import SentenceTransformer\nimport torch\n\nmodel = SentenceTransformer('/kaggle/input/bge-large-en-v1-5', trust_remote_code=True)\n\nGROUP = args.GROUP\nnum_1 = args.NUM_1\nnum_2 = args.NUM_2\n\nretrivals_groups = []\ntest_sorted_indices_search = []\nfor i in range(len(test_sorted_indices_topk)):\n    test_sorted_indices_topk_i = test_sorted_indices_topk[i].copy()\n    test_sorted_indices_topk_i = test_sorted_indices_topk_i[:num_1]\n    # test_sorted_indices_topk_i.reverse()\n    test_sorted_indices_search.append(test_sorted_indices_topk_i)\n    descriptions = []\n    for j in range(num_1):\n        descriptions.append(Misconception_id2name[test_sorted_indices_topk_i[j]])\n    retrival = \"\\n\".join([f\"{k+1}. {desc}\" for k, desc in enumerate(descriptions)])\n    retrivals_groups.append(retrival)\ntest_long['Retrival'] = retrivals_groups\ntest_long[\"Prompt\"] = test_long.apply(lambda row: apply_template(row, tokenizer), axis=1)\n\n# 生成每一组的Misconception_name\nconstruct_responses = llm.generate(\ntest_long[\"Prompt\"].values,\nvllm.SamplingParams(\n    n=1,  \n    top_p=0.8,  \n    temperature=0,  \n    seed=777, \n    skip_special_tokens=False,  \n    max_tokens=324, \n),\nuse_tqdm = True\n)\ngen_construct_texts = [x.outputs[0].text for x in construct_responses]\ngen_construct_texts = number2sentence(gen_construct_texts, test_sorted_indices_search, Misconception_id2name)\ntest_long[\"llmMisconception_clean\"] = gen_construct_texts\n\ntest_long_vec = model.encode(test_long['llmMisconception_clean'].tolist(), normalize_embeddings=True)\nmisconception_mapping_vec = model.encode(Misconception_df[\"MisconceptionName\"].to_list(), normalize_embeddings=True)\ntest_cos_sim_arr = cosine_similarity(test_long_vec, misconception_mapping_vec)\ntest_sorted_indices = np.argsort(-test_cos_sim_arr, axis=1)\n\nMisconception_2d = test_sorted_indices[:, :1].tolist()\n\nfor i in range(len(Misconception_2d)):\n    if Misconception_2d[i][0] in test_sorted_indices_topk[i]:\n        test_sorted_indices_topk[i].remove(Misconception_2d[i][0])\n\nfor g in range(1, GROUP):\n    retrivals_groups = []\n    test_sorted_indices_search = []\n    for i in range(len(test_sorted_indices_topk)):\n        test_sorted_indices_topk_i = test_sorted_indices_topk[i].copy()\n        if i == 1:\n            # test_sorted_indices_topk_i = test_sorted_indices_topk_i[:num_1]\n            test_sorted_indices_topk_i = test_sorted_indices_topk_i[:num_2]\n        else:\n            test_sorted_indices_topk_i = test_sorted_indices_topk_i[:num_2]\n        test_sorted_indices_search.append(test_sorted_indices_topk_i)\n        descriptions = []\n        for j in range(num_2):\n            descriptions.append(Misconception_id2name[test_sorted_indices_topk_i[j]])\n        retrival = \"\\n\".join([f\"{k+1}. {desc}\" for k, desc in enumerate(descriptions)])\n        retrivals_groups.append(retrival)\n    test_long['Retrival'] = retrivals_groups\n    test_long[\"Prompt\"] = test_long.apply(lambda row: apply_template(row, tokenizer), axis=1)\n    \n    # 生成每一组的Misconception_name\n    construct_responses = llm.generate(\n    test_long[\"Prompt\"].values,\n    vllm.SamplingParams(\n        n=1,  \n        top_p=0.8,  \n        temperature=0,  \n        seed=777, \n        skip_special_tokens=False,  \n        max_tokens=324, \n    ),\n    use_tqdm = True\n    )\n    gen_construct_texts = [x.outputs[0].text for x in construct_responses]\n    gen_construct_texts = number2sentence(gen_construct_texts, test_sorted_indices_search, Misconception_id2name)\n    test_long[\"llmMisconception_clean\"] = gen_construct_texts\n\n    # 计算每一次的余弦相似度\n    test_long_vec = model.encode(test_long['llmMisconception_clean'].tolist(), normalize_embeddings=True)\n    test_cos_sim_arr = cosine_similarity(test_long_vec, misconception_mapping_vec)\n    test_sorted_indices = np.argsort(-test_cos_sim_arr, axis=1)\n    \n    if g != GROUP-1:\n        test_sorted_indices_group = test_sorted_indices[:, :1].tolist()\n        for i in range(len(Misconception_2d)):\n            if test_sorted_indices_group[i][0] in test_sorted_indices_topk[i]:\n                test_sorted_indices_topk[i].remove(test_sorted_indices_group[i][0])\n            Misconception_2d[i].extend(test_sorted_indices_group[i])\n    else:\n        test_sorted_indices_group = test_sorted_indices[:, :30].tolist()\n        for i in range(len(Misconception_2d)):\n              temp_list = test_sorted_indices_group[i].copy()\n              test_sorted_indices_group[i] = [item for item in temp_list if item not in Misconception_2d[i]]\n              Misconception_2d[i].extend(test_sorted_indices_group[i])\n\nfor i in range(len(Misconception_2d)):\n    seen = set()\n    temp_list = Misconception_2d[i].copy()\n    Misconception_2d[i] = [item for item in temp_list if not (item in seen or seen.add(item))]\n\nllm_result_2d = test_long['MisconceptionId_topk'].tolist()\nllm_result_2d = [item.tolist() for item in llm_result_2d]\n# print(llm_result_2d)\n\nMisconception_2d_emsemble = []\nfor i in range(len(Misconception_2d)):\n    llm_result_1d = llm_result_2d[i]\n    Misconception_1d = Misconception_2d[i]\n    Misconception_1d = Misconception_1d[:GROUP]\n    for misconception in Misconception_1d:\n        if misconception in llm_result_1d:\n            llm_result_1d.remove(misconception)\n    Misconception_1d.extend(llm_result_1d)\n    Misconception_2d_emsemble.append(Misconception_1d)\n\ntest_long[\"MisconceptionId\"] = Misconception_2d_emsemble\ntest_long.reset_index(drop=True, inplace=True)\n\ntest_long[\"MisconceptionId\"] = test_long[\"MisconceptionId\"].apply(lambda x: ' '.join(map(str, x)))\nsubmission = test_long[[\"QuestionId_Answer\", \"MisconceptionId\"]].reset_index(drop=True)\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(submission)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python run_vllm.py","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}